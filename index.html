<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Data by Daniel</title>

  <!-- AOS for scroll animations -->
  <link rel="stylesheet" href="https://unpkg.com/aos@2.3.1/dist/aos.css" />

  <!-- Plotly for interactive charts -->
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

  <!-- Main CSS -->
  <link rel="stylesheet" href="styles.css" />
</head>
<body>

  <!-- Fixed Navigation -->
  <header class="site-header">
    <div class="nav-brand">Data by Daniel</div>
    <nav class="nav-links">
      <a href="#resume">Resume</a>
      <a href="#education">Education</a>
      <a href="#skills">Skills</a>
      <a href="#certifications">Certs</a>
      <a href="#projects">Projects</a>
      <a href="#contact">Contact</a>
    </nav>
  </header>

  <!-- Hero / Call to Action -->
  <section class="hero-section" id="top">
    <div class="hero-content" data-aos="fade-up" data-aos-duration="900">
      <h1 class="hero-title">Crafting Insights from Everyday Data</h1>
      <p class="hero-subtitle">
        Hi, I'm Daniel! If you're reading this, there's a chance that you may be hiring 
        a data analyst, project manager, or any of the wide variety of roles that exist 
        in between. If so, scroll down to find out why, in my humble opinion, 
        you should consider hiring me!
      </p>
      <p class="cta-contact">
        <strong>Email:</strong> <a href="mailto:dmindlin824@gmail.com">dmindlin824@gmail.com</a> &nbsp;|&nbsp; 
        <strong>Phone:</strong> <a href="tel:8186658871">818-665-8871</a>
      </p>
      <a href="#resume" class="cta-button" data-aos="zoom-in" data-aos-duration="600" data-aos-delay="300">
        Discover My Work
      </a>
    </div>
    <!-- Hero Background Image (relative path for GitHub Pages) -->
    <img 
      src="./images/hans-jurgen-mager-qQWV91TTBrE-unsplash.jpg"
      alt="Hero Background Image" 
      class="hero-bg" 
    />
  </section>

  <!-- Resume Section (Dynamic Timeline) -->
  <section class="resume-section" id="resume">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Professional Timeline</h2>
      <p>
        Below is a chronological look at my experience, covering key data roles,
        small-scale project coordination responsibilities, and an overall perspective 
        on how analytics can transform practical problems into real solutions.
      </p>

      <div class="timeline">
        <!-- NationsBenefits -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="100">
          <div class="timeline-content">
            <h3>NationsBenefits (October 2024 - February 2025)</h3>
            <p class="timeline-sub">Senior Data Analyst</p>
            <p>
              Venturing into the healthcare realm, I constructed SQL/Python pipelines 
              that cleansed and modeled large-scale data for Elevance and Aetna, boosting 
              data accuracy by ~20%. In my junior capacity, I shouldered day-to-day planning 
              responsibilities—coordinating minor sprints, aligning tasks with the business 
              team, and ensuring that each data milestone was appropriately validated. 
              My Power BI and Tableau dashboards supported multi-million-dollar strategic 
              initiatives, improving efficiency by ~15%.
            </p>
            <p>
              I also implemented iterative transformations with dbt (a data build tool)
              to streamline data extraction, raising processing efficiency another ~15–20%.
              During this period, I learned to blend standard regression and hypothesis 
              testing (pandas, scikit-learn, SciPy) into everyday workflows, accelerating 
              data-driven decisions at the C-level. Although I was an associate-level 
              contributor, I actively planned pipeline enhancement tasks to ensure on-time 
              delivery and a coherent data journey from ingestion to final insights.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- CDI Advisors -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="200">
          <div class="timeline-content">
            <h3>CDI Advisors (January 2024 - September 2024)</h3>
            <p class="timeline-sub">Senior Data Analyst (Contract)</p>
            <p>
              As a contract Senior Data Analyst working alongside a tight-knit team, 
              I focused on forecasting solutions using Python libraries like pandas, 
              scikit-learn, and XGBoost, cutting forecast errors by about 10%. 
              I introduced SAS-based accuracy checks to refine existing forecasting processes, 
              and used dbt in conjunction with Airflow on a modest AWS/Snowflake environment 
              for ~20–25% overall workflow efficiency gains.
            </p>
            <p>
              My responsibilities included running short weekly stand-ups, setting 
              small deadlines, and verifying deliverables matched stakeholder expectations.
              This experience taught me how iterative improvements and consistent 
              communication can elevate forecast fidelity—even in a local or mid-scale environment.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- Royal Caribbean Group -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="300">
          <div class="timeline-content">
            <h3>Royal Caribbean Group (September 2023 - December 2023)</h3>
            <p class="timeline-sub">Senior Data Analyst (Contract)</p>
            <p>
              At Royal Caribbean Group, I devised SQL/Python revenue models for a fleet 
              of 26 ships, elevating demand projection accuracy by ~15–20%. I also 
              scripted Python-based ticket pricing updates, reducing manual input by ~30–40% 
              and enhancing operational efficiency by ~10–15%. In my associate role, I 
              tracked each deliverable's progress (like implementing advanced window 
              functions) in short sprints, ensuring code quality and data integrity 
              before deployment.
            </p>
            <p>
              This environment prioritized fast decisions, so I coordinated with 
              operations to confirm pipeline readiness each week. By bridging data 
              engineering tasks and simple project management duties, I helped deliver 
              near real-time fare adjustments and quickly capitalized on new 
              revenue opportunities.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- CVS Health -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="400">
          <div class="timeline-content">
            <h3>CVS Health (May 2022 - January 2023)</h3>
            <p class="timeline-sub">Analytics Consultant</p>
            <p>
              Taking on a consulting role at CVS Health, I worked with SQL (including advanced
              window functions, CTEs) and Python-based libraries (pandas, scikit-learn)
              to optimize multi-team data workflows on local or minimal hardware setups. 
              My duties involved clarifying each ingestion or transformation task 
              in a backlog and verifying final outputs within specified deadlines.
            </p>
            <p>
              By deploying scalable data pipelines (Airflow, dbt), I reduced manual data handling, 
              allowing teams to focus on strategic data usage. I further integrated ML models 
              (TensorFlow, XGBoost) for forecasting. While others determined the vision, 
              I collaborated closely with them, ensuring my small-scale management 
              approach kept daily tasks on target, culminating in a better overall pipeline 
              for timely analytics.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- Qvest.US -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="500">
          <div class="timeline-content">
            <h3>Qvest.US (July 2021 - March 2022)</h3>
            <p class="timeline-sub">Consulting Analyst</p>
            <p>
              At Qvest.US, I stepped into an associate-level data analyst role supporting 
              SQL-driven data pipelines for cross-department Tableau/Power BI dashboards. 
              These pipelines fostered real-time insights for sales and operations. 
              My self-organized “micro-projects” for each enhancement ensured tasks remained 
              bite-sized and trackable, so stakeholders saw incremental gains every two weeks.
            </p>
            <p>
              I also ran market and competitive analyses with Python scripts, 
              enabling data-backed strategy formation. Although I juggled 
              typical junior analytics tasks, I found that minimal but structured 
              project planning (like short stand-ups and Gantt charts) significantly 
              boosted visibility and maintained progress across concurrent tasks.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>
      </div>
    </div>
  </section>

  <!-- Education Section -->
  <section class="education-section" id="education">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Education</h2>
      <div class="edu-content">
        <div class="degree">
          <h3>Bachelor of Science in Statistics &amp; Bachelor of Arts in History</h3>
          <p class="university">University of California, Santa Barbara</p>
          <p class="edu-year">Graduated: August 2020</p>
        </div>
        <div class="degree">
          <h3>Master of Science in Business Analytics</h3>
          <p class="university">University of San Diego</p>
          <p class="edu-year">Graduated: May 2021</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Skills Section -->
  <section class="skills-section" id="skills">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Technical Skills</h2>
      <div class="skills-grid">
        <div class="skill-card" data-aos="fade-up" data-aos-delay="200">
          <h3>🖥 Programming</h3>
          <p>SQL, Python (pandas, NumPy, scikit-learn, TensorFlow)</p>
        </div>
        <div class="skill-card" data-aos="fade-up" data-aos-delay="300">
          <h3>📊 Data Visualization</h3>
          <p>Tableau, Power BI, Plotly, Matplotlib, Seaborn</p>
        </div>
        <div class="skill-card" data-aos="fade-up" data-aos-delay="400">
          <h3>💾 Big Data &amp; Cloud</h3>
          <p>Spark, Snowflake, dbt, Airflow, AWS, Azure, BigQuery</p>
        </div>
        <div class="skill-card" data-aos="fade-up" data-aos-delay="500">
          <h3>🔗 Data Engineering</h3>
          <p>ETL/ELT, API Integration, Web Scraping</p>
        </div>
        <div class="skill-card" data-aos="fade-up" data-aos-delay="600">
          <h3>🧠 Machine Learning</h3>
          <p>Classification Models, Forecasting, NLP</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Certifications Section -->
  <section class="certifications-section" id="certifications">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Certifications</h2>
      <div class="cert-grid">
        <div class="cert-card">
          <h3>Salesforce Certified Administrator</h3>
          <p>Salesforce</p>
        </div>
        <div class="cert-card">
          <h3>Salesforce Certified Advanced Administrator</h3>
          <p>Salesforce</p>
        </div>
        <div class="cert-card">
          <h3>AWS Certified Cloud Practitioner</h3>
          <p>Amazon Web Services</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Projects Section -->
  <section id="projects">
    <!-- Project 1 Container -->
    <section class="project-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Project 1: Sentiment Analysis on 1.6 Million Tweets</h2>
      <p>
        <strong>Project Overview:</strong> In this personal exercise, I aimed to uncover emotional 
        trends in a massive collection of tweets. By meticulously cleaning text data and 
        employing logistic regression, I explored how sentiment fluctuates over time in 
        response to key events or viral topics.
      </p>
      <p>
        <strong>Process:</strong> After normalizing tweets (removing special characters, 
        tokenizing words, applying TF-IDF), I trained the model with cross-validation 
        to optimize accuracy. This approach highlighted how strongly negative tweets 
        often spiked around controversial subjects. Additionally, an overlay of retweet volume 
        revealed that viral negativity often garners outsized engagement.
      </p>
      <p>
        <strong>Results &amp; Insights:</strong> The final model exceeded 90% accuracy 
        on validation data, confirming its reliability in gauging overall sentiment. 
        These insights pointed to a correlation between polarizing content and 
        higher user engagement, underscoring how emotional language shapes 
        social media dynamics.
      </p>
      <div id="textAnalysisChart" class="vis-container"></div>
      <p>
        <em>Visualization: Average Sentiment &amp; Retweet Volume</em><br/>
        The left axis here displays sentiment (-1 to +1), while the right axis 
        captures total retweet volume. Spikes in negativity consistently align with 
        heavy retweet activity, illustrating how emotive content proliferates faster online.
      </p>
    </section>

    <!-- Project 2 Container -->
    <section class="project-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Project 2: Time Series Forecasting for Retail Demand</h2>
      <p>
        <strong>Project Overview:</strong> Using detailed sales data covering multiple 
        product lines over two years, I developed a forecasting system to anticipate 
        monthly demand shifts. The goal was to reduce last-minute rush shipping and 
        better align promotional timing.
      </p>
      <p>
        <strong>Process:</strong> I cleaned the dataset (removing anomalies), introduced 
        features like promotional flags and day-of-week indicators, and tested multiple 
        modeling strategies. An ensemble of Prophet (for seasonality) and RandomForestRegressor 
        (for non-linear interactions) outperformed baseline ARIMA, validated via 
        rolling window back-testing.
      </p>
      <p>
        <strong>Results &amp; Insights:</strong> The final model cut mean absolute error 
        by ~17% over naive methods, enabling managers to reorder more precisely. 
        Detailed error analysis revealed certain categories exhibited irregular surges 
        around holidays or marketing pushes, highlighting the need for real-time forecast 
        updates during those periods.
      </p>
      <div id="forecastChart" class="vis-container"></div>
      <p>
        <em>Visualization: Actual &amp; Forecasted Monthly Sales (Confidence Bands)</em><br/>
        Bars show actual sales across three product categories, while lines depict 
        predicted volumes. The shaded areas around Category A’s forecast illustrate 
        a 95% confidence interval, highlighting potential variance in demand.
      </p>
    </section>

    <!-- Project 3 Container -->
    <section class="project-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Project 3: Advanced Customer Segmentation in E-Commerce</h2>
      <p>
        <strong>Project Overview:</strong> Merging session logs with transaction histories, 
        I aimed to cluster users based on their buying patterns, frequency, and average 
        order values. The resulting segments guided marketing teams toward more effective 
        loyalty and upsell strategies.
      </p>
      <p>
        <strong>Process:</strong> I engineered features like recency, cart abandonment rates, 
        and total spend, then compared algorithms (K-Means, DBSCAN, hierarchical) using 
        silhouette scores. K-Means with five clusters offered the best interpretability. 
        Each segment was labeled by characteristic behaviors—for instance, 
        “Mid-Freq, Mid-Spend” or “High-Freq, High-Spend,” enabling targeted approaches.
      </p>
      <p>
        <strong>Results &amp; Insights:</strong> Marketing campaigns tailored to each cluster 
        boosted email open rates by ~18%. Observing user migration across segments also 
        helped detect churn risk or ascendant buying patterns. The net outcome: 
        a more nuanced view of consumer habits, fueling data-driven retargeting 
        and retention efforts.
      </p>
      <div id="clusteringChart" class="vis-container"></div>
      <p>
        <em>Visualization: Five Clusters by Purchase Frequency &amp; Average Spend</em><br/>
        A color-coded scatter plot reveals the distinct groups, 
        with each cluster demonstrating unique spending and frequency metrics. 
        Labeled centroids help pinpoint typical buyer behaviors, 
        informing more nuanced marketing.
      </p>
    </section>

    <!-- Project 4 Container -->
    <section class="project-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Project 4: Real-Time IoT Anomaly Detection</h2>
      <p>
        <strong>Project Overview:</strong> I developed a pipeline to identify and visualize 
        anomalies in IoT sensor data in near real-time. By proactively spotting erratic readings, 
        the system provided early warnings of potential hardware failures or hazardous conditions 
        in connected devices (e.g., smart thermostats or industrial machinery).
      </p>
      <p>
        <strong>Process:</strong> I configured a local environment using Python for ingestion 
        and quick ETL tasks, alongside Kafka for data queuing. Each sensor feed transmitted 
        temperature, vibration, and humidity metrics. An isolation forest model 
        (an algorithm that flags outliers in multi-dimensional data) identified 
        unusual spikes or dips. A threshold-based approach triggered Slack alerts 
        when sensor values breached normal operating ranges.
      </p>
      <p>
        <strong>Results &amp; Insights:</strong> The pipeline flagged incremental deviations 
        that manual checks would likely miss, cutting mean response time to abnormal events 
        by 40%. Cluster-based analysis revealed that anomalies often involved simultaneous 
        temperature and vibration jumps, reinforcing the importance of correlating sensor 
        variables. Overall, this system helped detect mechanical issues earlier, 
        reducing downtime and repair costs.
      </p>
      <div id="iotAnomalyChart" class="vis-container"></div>
      <p>
        <em>Visualization: Temperature &amp; Vibration with Threshold Lines</em><br/>
        The chart above shows both temperature (left axis) and vibration (right axis), 
        with dashed lines marking upper and lower thresholds. Any data points that cross 
        these thresholds or exhibit extreme combined behaviors are flagged as anomalies, 
        highlighting critical events in real-time.
      </p>
    </section>
    <!-- Project 5 Container -->
<section class="project-container" data-aos="fade-up" data-aos-duration="900">
  <h2>Project 5: Credit Card Fraud Detection</h2>
  <p>
    <strong>Project Overview:</strong> In this self-directed project, I explored classification 
    techniques to identify potentially fraudulent transactions. Since fraud typically accounts 
    for a tiny percentage of all credit card activity, my primary challenge was dealing with 
    this highly imbalanced dataset to ensure legitimate transactions weren’t frequently misclassified 
    while still catching true fraud.
  </p>
  <p>
    <strong>Process:</strong>
    First, I split the transaction data into training and test sets. I then introduced specialized 
    methods for imbalanced learning, such as <em>SMOTE</em> (Synthetic Minority Over-Sampling Technique) 
    to replicate fraud examples more evenly, and <em>undersampling</em> of legitimate transactions 
    to maintain a workable ratio. I ran multiple experiments using <strong>RandomForestClassifier</strong> 
    and <strong>XGBoost</strong> with a heavy focus on the <em>ROC–AUC</em> (Receiver Operating 
    Characteristic–Area Under the Curve) as my primary evaluation metric, because it illustrates 
    how well the model distinguishes fraud from legitimate transactions at various thresholds.
  </p>
  <p>
    <strong>Results &amp; Insights:</strong>
    My final ensemble model achieved ~98% ROC–AUC, reducing missed fraud (false negatives) significantly. 
    I also analyzed false positives (legitimate transactions flagged as fraud), finding patterns like 
    legitimate overseas travel or sporadic big-ticket purchases. By applying domain knowledge 
    to these borderline cases, I refined the pipeline to avoid inconveniencing users who legitimately 
    exhibit “abnormal” patterns. Overall, this approach showcased how careful handling of minority classes 
    can deliver high-confidence fraud alerts without overwhelming call centers or negatively impacting 
    customer experiences.
  </p>

  <!-- Chart Container -->
  <div id="fraudChart" class="vis-container"></div>
  
  <p>
    <em>Visualization: Confusion Matrix &amp; ROC Curve</em><br/>
    Here, you can see a <strong>confusion matrix</strong> (a table comparing predicted vs. actual classes), 
    which helps measure how many transactions were correctly or incorrectly categorized. A <em>False Positive</em> 
    occurs when a legitimate transaction is wrongly flagged as fraud, while a <em>False Negative</em> happens 
    if a fraudulent transaction is incorrectly labeled as legitimate. On the right is the <strong>ROC curve</strong> 
    (Receiver Operating Characteristic curve), plotting the <em>True Positive Rate</em> (the fraction of all 
    fraud that is correctly caught) against the <em>False Positive Rate</em> (the fraction of all legitimate 
    transactions that are incorrectly flagged). The further the ROC curve pushes toward the top-left, the 
    better the model is at distinguishing fraud from legitimate transactions across different thresholds. 
    This curve’s <em>Area Under the Curve (AUC)</em> is a consolidated measure of the model’s overall 
    separating power.
  </p>
</section>

  
  </section>

  <!-- Contact Section -->
  <section class="contact-section" id="contact">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Contact</h2>
      <p>
        I appreciate your interest in my portfolio. If you have a data challenge 
        that could benefit from local, Python-based development and thorough 
        analytics, please reach out to learn more or to discuss possible 
        collaborations.
      </p>
      <p>
        <strong>Email:</strong> <a href="mailto:dmindlin824@gmail.com">dmindlin824@gmail.com</a><br/>
        <strong>Phone:</strong> <a href="tel:8186658871">818-665-8871</a><br/>
        <strong>LinkedIn:</strong> 
        <a href="https://www.linkedin.com/in/daniel-mindlin" target="_blank">
          linkedin.com/in/daniel-mindlin</a>
      </p>
    </div>
  </section>

  <!-- Footer -->
  <footer class="site-footer" data-aos="fade-up" data-aos-duration="700">
    <p>&copy; 2025 Data by Daniel. All rights reserved.</p>
  </footer>

  <!-- AOS JS -->
  <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
  <!-- Main JS (with Plotly chart logic for all 4 projects) -->
  <script src="script.js"></script>
</body>
</html>
