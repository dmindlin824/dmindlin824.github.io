<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Please Hire Daniel</title>

  <!-- AOS for scroll animations -->
  <link rel="stylesheet" href="https://unpkg.com/aos@2.3.1/dist/aos.css" />

  <!-- Plotly for interactive charts -->
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

  <!-- Main CSS -->
  <link rel="stylesheet" href="styles.css" />
</head>
<body>

  <!-- Fixed Navigation -->
  <header class="site-header">
    <div class="nav-brand">Please Hire Daniel</div>
    <nav class="nav-links">
      <a href="#resume">Resume</a>
      <a href="#projects">Projects</a>
      <a href="#contact">Contact</a>
    </nav>
  </header>

  <!-- Hero / Call to Action -->
  <section class="hero-section" id="top">
    <div class="hero-content" data-aos="fade-up" data-aos-duration="900">
      <h1 class="hero-title">Crafting Insights from Everyday Data</h1>
      <p class="hero-subtitle">
       Hi, I'm Daniel! If you're reading this, there's a chance that you may be hiring a data analyst, project manager, or any of the wide variety of roles that exist in between. If so, scroll down to find out why, in my humble opinion, you should consider hiring me! 
      </p>
      <p class="cta-contact">
        <strong>Email:</strong> <a href="mailto:dmindlin824@gmail.com">dmindlin824@gmail.com</a> &nbsp;|&nbsp; 
        <strong>Phone:</strong> <a href="tel:8186658871">818-665-8871</a>
      </p>
      <a href="#resume" class="cta-button" data-aos="zoom-in" data-aos-duration="600" data-aos-delay="300">
        Discover My Work
      </a>
    </div>
    <!-- Local Hero Background Image -->
<img 
  src="images/hans-jurgen-mager-qQWV91TTBrE-unsplash.jpg"
  alt="Hero Background Image" 
  class="hero-bg" 
/>
  </section>

  <!-- Resume Section (Dynamic Timeline) -->
  <section class="resume-section" id="resume">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Professional Timeline</h2>
      <p>
        Below is a chronological look at my experience, covering key data roles, 
        skill evolution, and overall perspective on how analytics can 
        transform practical problems into real solutions.
      </p>

      <div class="timeline">
        <!-- NationsBenefits -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="100">
          <div class="timeline-content">
            <h3>NationsBenefits (October 2024 - February 2025)</h3>
            <p class="timeline-sub">Senior Data Analyst</p>
            <p>
              I tackled large healthcare data challenges by constructing a pipeline 
              (SQL, Python, dbt) for patient records, raising data accuracy by ~20%. 
              Concurrently, I maintained interactive dashboards (Power BI, Tableau) 
              that boosted operational efficiency ~15%. Testing each data transformation 
              thoroughly guaranteed stable output and built trust in my final analyses.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- CDI Advisors -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="200">
          <div class="timeline-content">
            <h3>CDI Advisors (January 2024 - September 2024)</h3>
            <p class="timeline-sub">Senior Data Analyst (Contract)</p>
            <p>
              My work focused on forecasting models (using Python libraries such as scikit-learn 
              for machine learning), trimming prediction errors ~10%. I deployed efficient 
              ETL solutions (dbt, Airflow) on a small AWS instance, achieving ~20–25% 
              total workflow efficiency. Visual dashboards helped finance and marketing 
              quickly interpret trends and identify where to act next.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- Royal Caribbean Group -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="300">
          <div class="timeline-content">
            <h3>Royal Caribbean Group (September 2023 - December 2023)</h3>
            <p class="timeline-sub">Senior Data Analyst (Contract)</p>
            <p>
              I enhanced revenue forecasting (SQL/Python) for a fleet of 26 ships, 
              raising accuracy ~15–20%. I also automated ticket pricing operations 
              (via daily Python scripts) that cut manual updates by ~30–40%. 
              This role showed me how crucial real-time data is for an environment 
              that relies on quick decisions, even though everything ran 
              on standard Python libraries in a local or small-scale server setting.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- CVS Health -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="400">
          <div class="timeline-content">
            <h3>CVS Health (May 2022 - January 2023)</h3>
            <p class="timeline-sub">Analytics Consultant</p>
            <p>
              Using SQL (window functions, CTEs) and Python, I delivered 
              advanced data science products on a local environment with 
              carefully curated data. This approach maximized speed and 
              clarity for multi-team processing. I also set up a library 
              of scikit-learn models for better forecasting. 
              The net effect was a more efficient pipeline that required 
              minimal overhead, even on modest hardware.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- Qvest.US -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="500">
          <div class="timeline-content">
            <h3>Qvest.US (July 2021 - March 2022)</h3>
            <p class="timeline-sub">Consulting Analyst</p>
            <p>
              I created SQL-based data pipelines for organizational dashboards, 
              enabling cross-department synergy. Defining KPIs (key performance 
              indicators, i.e., important metrics that show how well a business 
              is doing) with managers helped me produce high-impact visualizations, 
              all built with standard Python and common libraries that can run on 
              any PC with minimal fuss.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- Education & Skills -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="600">
          <div class="timeline-content">
            <h3>Education & Skills</h3>
            <p class="timeline-sub">Core Foundations</p>
            <p>
              <strong>B.S. in Statistics & B.A. in History</strong>, UC Santa Barbara (August 2020)<br/>
              <strong>M.S. in Business Analytics</strong>, University of San Diego (May 2021)
            </p>
            <p>
              <strong>Skills:</strong> SQL, Python, dbt, Airflow, scikit-learn, 
              Tableau, Power BI, AWS, Azure, BigQuery, ETL/ELT, Data Visualization
            </p>
            <p>
              <strong>Certifications:</strong> Salesforce Certified Administrator, 
              Salesforce Certified Advanced Administrator, AWS Certified Cloud Practitioner
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>
      </div>
    </div>
  </section>

  <!-- Projects Section (Kaggle-based, local environment, well-explained) -->
  <section class="projects-section" id="projects">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Independent Projects</h2>
      <p>
Every dataset tells a story—sometimes it’s about shifting consumer behavior, sometimes it's a pattern hidden in millions of transactions, and sometimes it’s the unexpected signal buried in the noise. The projects below are deep dives into real-world datasets, where I’ve unraveled trends, built predictive models, and extracted meaningful insights from raw numbers.

Each project tackles a unique analytical challenge, from forecasting demand to deciphering sentiment at scale. The goal isn’t just to process data—it’s to illuminate patterns, solve problems, and turn numbers into decisions.

What follows are three detailed case studies. You’ll see how I approach complex problems, the logic behind my analyses, and the key takeaways that transform data into strategy. Technical concepts are explained along the way, ensuring that whether you're a fellow data analyst or just curious about the power of analytics, you’ll walk away with a clear understanding of how these projects deliver real, actionable insights.
      </p>

      <!-- Project 1: Sentiment Analysis on Kaggle Tweets -->
      <div class="project-card" data-aos="fade-up" data-aos-duration="900" data-aos-delay="100">
        <div class="project-details animated-panel">
          <h3>1. Sentiment Analysis on ~1.6M Tweets</h3>
          <p>
            <strong>Dataset:</strong> A Kaggle CSV file with ~1.6 million tweets labeled with 
            positive or negative sentiment. All processing was done on a single PC 
            using Python, pandas (a library for data manipulation in Python), and 
            scikit-learn (a machine learning toolkit for Python).
          </p>
          <p>
            <strong>Details:</strong> I cleaned the raw text by removing stopwords 
            (common words like “the” or “and” that add little meaning), 
            applied tokenization (splitting text into words), and used TF-IDF 
            (term frequency-inverse document frequency, a measure of how important 
            a word is in a set of documents) to transform text into numerical features. 
            I then trained logistic regression (a model that predicts a binary outcome) 
            to classify sentiment. 
          </p>
          <p>
            <strong>Outcome:</strong> After multiple cross-validation (a technique that 
            tests how well a model generalizes to unseen data by repeatedly splitting 
            the dataset) cycles, I achieved ~85% accuracy on a hold-out set. 
            This experience taught me how to handle data quality issues 
            (like emojis or slang) in a local environment with limited memory.
          </p>
          <div id="textAnalysisChart" class="vis-container"></div>
          <p>
            <em>Visualization (Sample):</em> The chart above models changes in 
            average sentiment scores across different months in the dataset. 
            Each data point is annotated with key tweet volumes for clarity.
          </p>
        </div>
      </div>

      <!-- Project 2: Time Series Forecasting with Retail Dataset -->
      <div class="project-card" data-aos="fade-up" data-aos-duration="900" data-aos-delay="200">
        <div class="project-details animated-panel">
          <h3>2. Time Series Forecasting on Retail Sales</h3>
          <p>
            <strong>Dataset:</strong> A Kaggle retail dataset with daily sales figures 
            for multiple product categories, stored in a CSV. I loaded the data locally 
            using Python and performed time series analysis to predict future sales.
          </p>
          <p>
            <strong>Details:</strong> I created features for promotions, day-of-week, 
            and rolling averages (averages computed over a specific time window, 
            like the last 7 days). I then selected a RandomForestRegressor 
            (a model that builds multiple decision trees and aggregates their predictions) 
            to capture complex patterns that a simpler model might miss. 
            I used back-testing (a method to test model performance on 
            a sequence of past data) to confirm consistency. 
          </p>
          <p>
            <strong>Outcome:</strong> Achieved a ~12% reduction in mean absolute error 
            (MAE, an average of absolute differences between predictions and actuals) 
            compared to a naive seasonal baseline. This improvement reinforced 
            how local data transformations and feature engineering can power 
            advanced forecasting, even without massive computational clusters.
          </p>
          <div id="forecastChart" class="vis-container"></div>
          <p>
            <em>Visualization (Sample):</em> The bar chart shows actual vs. predicted 
            weekly volumes for a top product category. Error bars (thin lines 
            that represent uncertainty in each prediction) highlight 
            where the model performed well and where it struggled.
          </p>
        </div>
      </div>

      <!-- Project 3: Customer Segmentation on E-Commerce Data -->
      <div class="project-card" data-aos="fade-up" data-aos-duration="900" data-aos-delay="300">
        <div class="project-details animated-panel">
          <h3>3. Customer Segmentation from E-Commerce Logs</h3>
          <p>
            <strong>Dataset:</strong> A Kaggle e-commerce log containing anonymized user IDs, 
            transaction amounts, and browsing patterns. The total file size was manageable 
            on my personal laptop (around 500 MB of compressed CSV).
          </p>
          <p>
            <strong>Details:</strong> I engineered features reflecting purchase frequency, 
            average cart size, and recency (how many days since last purchase). 
            I tried multiple clustering algorithms, including K-Means 
            (a method of partitioning data into a chosen number k of clusters) 
            and DBSCAN (a density-based approach that groups together points that are closely packed). 
            I used the silhouette score (a measure of how distinct each cluster is) 
            to decide final cluster counts. 
          </p>
          <p>
            <strong>Outcome:</strong> Grouping customers by shared behaviors 
            allowed me to suggest different promotions for high-value segments. 
            This entire procedure exemplified how local CSV cleaning, 
            standard Python tools, and iterative testing can yield 
            robust marketing insights without specialized infrastructure.
          </p>
          <div id="clusteringChart" class="vis-container"></div>
          <p>
            <em>Visualization (Sample):</em> The scatter plot above shows how 
            customers clustered by purchase frequency (X-axis) and average spend 
            (Y-axis), with each cluster color-coded to indicate shared behaviors. 
            Labels highlight each segment’s main characteristic, 
            guiding targeted product recommendations.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Contact Section -->
  <section class="contact-section" id="contact">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Contact</h2>
      <p>
        I appreciate your interest in my portfolio. If you have a data challenge 
        that could benefit from local, Python-based development and thorough 
        analytics, please reach out to learn more or to discuss possible 
        collaborations.
      </p>
      <p>
        <strong>Email:</strong> <a href="mailto:dmindlin824@gmail.com">dmindlin824@gmail.com</a><br/>
        <strong>Phone:</strong> <a href="tel:8186658871">818-665-8871</a><br/>
        <strong>LinkedIn:</strong> 
        <a href="https://www.linkedin.com/in/daniel-mindlin" target="_blank">
          linkedin.com/in/daniel-mindlin</a>
      </p>
    </div>
  </section>

  <!-- Footer -->
  <footer class="site-footer" data-aos="fade-up" data-aos-duration="700">
    <p>&copy; 2025 Please Hire Daniel. All rights reserved.</p>
  </footer>

  <!-- AOS JS -->
  <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
  <!-- Main JS -->
  <script src="script.js"></script>

</body>
</html>
