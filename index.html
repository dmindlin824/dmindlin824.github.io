<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Data by Daniel</title>

  <!-- AOS for scroll animations -->
  <link rel="stylesheet" href="https://unpkg.com/aos@2.3.1/dist/aos.css" />

  <!-- Plotly for interactive charts -->
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

  <!-- Main CSS -->
  <link rel="stylesheet" href="styles.css" />
</head>
<body>

  <!-- Fixed Navigation -->
  <header class="site-header">
    <div class="nav-brand">Data by Daniel</div>
    <nav class="nav-links">
      <a href="#resume">Resume</a>
      <a href="#projects">Projects</a>
      <a href="#contact">Contact</a>
    </nav>
  </header>

  <!-- Hero / Call to Action -->
  <section class="hero-section" id="top">
    <div class="hero-content" data-aos="fade-up" data-aos-duration="900">
      <h1 class="hero-title">Crafting Insights from Everyday Data</h1>
      <p class="hero-subtitle">
        Hi, I'm Daniel! If you're reading this, there's a chance that you may be hiring 
        a data analyst, project manager, or any of the wide variety of roles that exist in 
        between. If so, scroll down to find out why, in my humble opinion, you should consider hiring me!
      </p>
      <p class="cta-contact">
        <strong>Email:</strong> <a href="mailto:dmindlin824@gmail.com">dmindlin824@gmail.com</a> &nbsp;|&nbsp; 
        <strong>Phone:</strong> <a href="tel:8186658871">818-665-8871</a>
      </p>
      <a href="#resume" class="cta-button" data-aos="zoom-in" data-aos-duration="600" data-aos-delay="300">
        Discover My Work
      </a>
    </div>
    <!-- Hero background image referencing your local folder -->
    <img 
      src="./images/hans-jurgen-mager-qQWV91TTBrE-unsplash.jpg"
      alt="Hero Background Image" 
      class="hero-bg" 
    />
  </section>

  <!-- Resume Section (Dynamic Timeline) -->
  <section class="resume-section" id="resume">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Professional Timeline</h2>
      <p>
        Below is a chronological look at my experience, covering key data roles,
        small-scale project coordination responsibilities, and an overall perspective 
        on how analytics can transform practical problems into real solutions.
      </p>

      <div class="timeline">
        <!-- NationsBenefits -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="100">
          <div class="timeline-content">
            <h3>NationsBenefits (October 2024 - February 2025)</h3>
            <p class="timeline-sub">Senior Data Analyst</p>
            <p>
              Venturing into the healthcare realm, I constructed SQL/Python pipelines 
              that cleansed and modeled large-scale data for Elevance and Aetna, boosting 
              data accuracy by ~20%. In my junior capacity, I shouldered day-to-day planning 
              responsibilities—coordinating minor sprints, aligning tasks with the business 
              team, and ensuring that each data milestone was appropriately validated. My 
              Power BI and Tableau dashboards supported multi-million-dollar strategic 
              initiatives, improving efficiency by ~15%.
            </p>
            <p>
              I also implemented iterative transformations with dbt (a data build tool)
              to streamline data extraction, raising processing efficiency another ~15–20%.
              During this period, I learned to blend standard regression and hypothesis 
              testing (pandas, scikit-learn, SciPy) into everyday workflows, accelerating 
              data-driven decisions at the C-level. Although I was an associate-level 
              contributor, I actively planned pipeline enhancement tasks to ensure on-time 
              delivery and a coherent data journey from ingestion to final insights.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- CDI Advisors -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="200">
          <div class="timeline-content">
            <h3>CDI Advisors (January 2024 - September 2024)</h3>
            <p class="timeline-sub">Senior Data Analyst (Contract)</p>
            <p>
              As a contract Senior Data Analyst working alongside a tight-knit team, 
              I focused on forecasting solutions using Python libraries like pandas, 
              scikit-learn, and XGBoost, cutting forecast errors by about 10%. 
              I introduced SAS-based accuracy checks to refine existing forecasting processes, 
              and used dbt in conjunction with Airflow on a modest AWS/Snowflake environment 
              for ~20–25% overall workflow efficiency gains.
            </p>
            <p>
              My responsibilities included running short weekly stand-ups, setting 
              small deadlines, and verifying deliverables matched stakeholder expectations.
              This experience taught me how iterative improvements and consistent 
              communication can elevate forecast fidelity—even in a local or mid-scale environment.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- Royal Caribbean Group -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="300">
          <div class="timeline-content">
            <h3>Royal Caribbean Group (September 2023 - December 2023)</h3>
            <p class="timeline-sub">Senior Data Analyst (Contract)</p>
            <p>
              At Royal Caribbean Group, I devised SQL/Python revenue models for a fleet 
              of 26 ships, elevating demand projection accuracy by ~15–20%. I also 
              scripted Python-based ticket pricing updates, reducing manual input by ~30–40% 
              and enhancing operational efficiency by ~10–15%. In my associate role, I 
              tracked each deliverable's progress (like implementing advanced window 
              functions) in short sprints, ensuring code quality and data integrity 
              before deployment.
            </p>
            <p>
              This environment prioritized fast decisions, so I coordinated with 
              operations to confirm pipeline readiness each week. By bridging data 
              engineering tasks and simple project management duties, I helped deliver 
              near real-time fare adjustments and quickly capitalized on new 
              revenue opportunities.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- CVS Health -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="400">
          <div class="timeline-content">
            <h3>CVS Health (May 2022 - January 2023)</h3>
            <p class="timeline-sub">Analytics Consultant</p>
            <p>
              Taking on a consulting role at CVS Health, I worked with SQL (including advanced
              window functions, CTEs) and Python-based libraries (pandas, scikit-learn)
              to optimize multi-team data workflows on local or minimal hardware setups. 
              My duties involved clarifying each ingestion or transformation task 
              in a backlog and verifying final outputs within specified deadlines.
            </p>
            <p>
              By deploying scalable data pipelines (Airflow, dbt), I reduced manual data handling, 
              allowing teams to focus on strategic data usage. I further integrated ML models 
              (TensorFlow, XGBoost) for forecasting. While others determined the vision, 
              I collaborated closely with them, ensuring my small-scale management 
              approach kept daily tasks on target, culminating in a better overall pipeline 
              for timely analytics.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- Qvest.US -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="500">
          <div class="timeline-content">
            <h3>Qvest.US (July 2021 - March 2022)</h3>
            <p class="timeline-sub">Consulting Analyst</p>
            <p>
              At Qvest.US, I stepped into an associate-level data analyst role supporting 
              SQL-driven data pipelines for cross-department Tableau/Power BI dashboards. 
              These pipelines fostered real-time insights for sales and operations. 
              My self-organized “micro-projects” for each enhancement ensured tasks remained 
              bite-sized and trackable, so stakeholders saw incremental gains every two weeks.
            </p>
            <p>
              I also ran market and competitive analyses with Python scripts, 
              enabling data-backed strategy formation. Although I juggled 
              typical junior analytics tasks, I found that minimal but structured 
              project planning (like short stand-ups and Gantt charts) significantly 
              boosted visibility and maintained progress across concurrent tasks.
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>

        <!-- Education & Skills -->
        <div class="timeline-block" data-aos="fade-up" data-aos-duration="900" data-aos-delay="600">
          <div class="timeline-content">
            <h3>Education & Skills</h3>
            <p class="timeline-sub">Core Foundations</p>
            <p>
              <strong>B.S. in Statistics &amp; B.A. in History</strong>, UC Santa Barbara (August 2020)<br/>
              <strong>M.S. in Business Analytics</strong>, University of San Diego (May 2021)
            </p>
            <p>
              <strong>Skills:</strong> SQL, Python, dbt, Airflow, scikit-learn, 
              Tableau, Power BI, AWS, Azure, BigQuery, ETL/ELT, Data Visualization,
              Machine Learning
            </p>
            <p>
              <strong>Certifications:</strong> Salesforce Certified Administrator, 
              Salesforce Certified Advanced Administrator, AWS Certified Cloud Practitioner
            </p>
          </div>
          <div class="timeline-badge"></div>
        </div>
      </div>
    </div>
  </section>

  <!-- Projects Section -->
  <section class="projects-section" id="projects">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Independent Projects</h2>
      <p>
        Every dataset tells a story—sometimes it’s about shifting consumer behavior, 
        sometimes it's a pattern hidden in millions of transactions, and sometimes 
        it’s the unexpected signal buried in the noise. The projects below are deep 
        dives into real-world datasets, where I’ve unraveled trends, built predictive 
        models, and extracted meaningful insights from raw numbers.
      </p>
      <p>
        Each project tackles a unique analytical challenge, from forecasting demand 
        to deciphering sentiment at scale. The goal isn’t just to process data—it’s 
        to illuminate patterns, solve problems, and turn numbers into decisions.
      </p>
      <p>
        What follows are three detailed case studies. You’ll see how I approach complex 
        problems, the logic behind my analyses, and the key takeaways that transform 
        data into strategy. Technical concepts are explained along the way, ensuring 
        that whether you're a fellow data analyst or just curious about the power of 
        analytics, you’ll walk away with a clear understanding of how these projects 
        deliver real, actionable insights.
      </p>

      <!-- Project 1 -->
      <div class="project-card" data-aos="fade-up" data-aos-duration="900" data-aos-delay="100">
        <div class="project-details animated-panel">
          <h3>1. Sentiment Analysis on ~1.6M Tweets</h3>
          <p>
            <strong>Dataset:</strong> A Kaggle CSV file with ~1.6 million tweets labeled with 
            positive or negative sentiment. All processing was done on a single PC 
            using Python, pandas (a library for data manipulation in Python), and 
            scikit-learn (a machine learning toolkit for Python).
          </p>
          <p>
            <strong>Details:</strong> I cleaned the raw text by removing stopwords 
            (common words like “the” or “and” that add little meaning), 
            applied tokenization (splitting text into words), and used TF-IDF 
            (term frequency-inverse document frequency, a measure of how important 
            a word is in a set of documents) to transform text into numerical features. 
            I then trained logistic regression (a model that predicts a binary outcome) 
            to classify sentiment.
          </p>
          <p>
            <strong>Outcome:</strong> After multiple cross-validation (a technique that 
            tests how well a model generalizes to unseen data by repeatedly splitting 
            the dataset) cycles, I achieved ~85% accuracy on a hold-out set. 
            This experience taught me how to handle data quality issues 
            (like emojis or slang) in a local environment with limited memory.
          </p>
          <div id="textAnalysisChart" class="vis-container"></div>
          <p>
            <em>Visualization:</em> The chart above models changes in 
            average sentiment scores across different months in the dataset. 
            Each data point is annotated with key tweet volumes for clarity.
          </p>
        </div>
      </div>

      <!-- Project 2 -->
      <div class="project-card" data-aos="fade-up" data-aos-duration="900" data-aos-delay="200">
        <div class="project-details animated-panel">
          <h3>2. Time Series Forecasting on Retail Sales</h3>
          <p>
            <strong>Dataset:</strong> A Kaggle retail dataset with daily sales figures 
            for multiple product categories, stored in a CSV. I loaded the data locally 
            using Python and performed time series analysis to predict future sales.
          </p>
          <p>
            <strong>Details:</strong> I created features for promotions, day-of-week, 
            and rolling averages (averages computed over a specific time window, 
            like the last 7 days). I then selected a RandomForestRegressor 
            (a model that builds multiple decision trees and aggregates their predictions) 
            to capture complex patterns that a simpler model might miss. 
            I used back-testing (a method to test model performance on 
            a sequence of past data) to confirm consistency.
          </p>
          <p>
            <strong>Outcome:</strong> Achieved a ~12% reduction in mean absolute error 
            (MAE, an average of absolute differences between predictions and actuals) 
            compared to a naive seasonal baseline. This improvement reinforced 
            how local data transformations and feature engineering can power 
            advanced forecasting, even without massive computational clusters.
          </p>
          <div id="forecastChart" class="vis-container"></div>
          <p>
            <em>Visualization:</em> The bar chart shows actual vs. predicted 
            monthly volumes for a top product category. Error bars (thin lines 
            that represent uncertainty in each prediction) highlight 
            where the model performed well and where it struggled.
          </p>
        </div>
      </div>

      <!-- Project 3 -->
      <div class="project-card" data-aos="fade-up" data-aos-duration="900" data-aos-delay="300">
        <div class="project-details animated-panel">
          <h3>3. Customer Segmentation from E-Commerce Logs</h3>
          <p>
            <strong>Dataset:</strong> A Kaggle e-commerce log containing anonymized user IDs, 
            transaction amounts, and browsing patterns. The total file size was manageable 
            on my personal laptop (around 500 MB of compressed CSV).
          </p>
          <p>
            <strong>Details:</strong> I engineered features reflecting purchase frequency, 
            average cart size, and recency (how many days since last purchase). 
            I tried multiple clustering algorithms, including K-Means 
            (a method of partitioning data into a chosen number k of clusters) 
            and DBSCAN (a density-based approach that groups together points that are closely packed). 
            I used the silhouette score (a measure of how distinct each cluster is) 
            to decide final cluster counts. 
          </p>
          <p>
            <strong>Outcome:</strong> Grouping customers by shared behaviors 
            allowed me to suggest different promotions for high-value segments. 
            This entire procedure exemplified how local CSV cleaning, 
            standard Python tools, and iterative testing can yield 
            robust marketing insights without specialized infrastructure.
          </p>
          <div id="clusteringChart" class="vis-container"></div>
          <p>
            <em>Visualization:</em> The scatter plot above shows how 
            customers clustered by purchase frequency (X-axis) and average spend 
            (Y-axis), with each cluster color-coded to indicate shared behaviors. 
            Labels highlight each segment’s main characteristic, 
            guiding targeted product recommendations.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Contact Section -->
  <section class="contact-section" id="contact">
    <div class="section-container" data-aos="fade-up" data-aos-duration="900">
      <h2>Contact</h2>
      <p>
        I appreciate your interest in my portfolio. If you have a data challenge 
        that could benefit from local, Python-based development and thorough 
        analytics, please reach out to learn more or to discuss possible 
        collaborations.
      </p>
      <p>
        <strong>Email:</strong> <a href="mailto:dmindlin824@gmail.com">dmindlin824@gmail.com</a><br/>
        <strong>Phone:</strong> <a href="tel:8186658871">818-665-8871</a><br/>
        <strong>LinkedIn:</strong> 
        <a href="https://www.linkedin.com/in/daniel-mindlin" target="_blank">
          linkedin.com/in/daniel-mindlin</a>
      </p>
    </div>
  </section>

  <!-- Footer -->
  <footer class="site-footer" data-aos="fade-up" data-aos-duration="700">
    <p>&copy; 2025 Data by Daniel. All rights reserved.</p>
  </footer>

  <!-- AOS JS -->
  <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
  <!-- Main JS -->
  <script>
    AOS.init();

    // Sentiment Analysis on ~1.6M Tweets
    if (document.getElementById('textAnalysisChart')) {
      const months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun"];
      // Example sentiment averages (from -0.3 to +0.3)
      const avgSentiment = [-0.2, -0.1, 0, 0.1, 0.2, 0.25];
      // Example volumes (tweet counts)
      const tweetVolume = [220000, 230000, 270000, 300000, 330000, 360000];

      const sentimentTrace = {
        x: months,
        y: avgSentiment,
        name: "Avg Sentiment",
        mode: "lines+markers",
        yaxis: 'y1',
        line: { color: "#007AFF" }
      };

      const volumeTrace = {
        x: months,
        y: tweetVolume,
        name: "Tweet Volume",
        mode: "lines+markers",
        yaxis: 'y2',
        line: { color: "#34C759" }
      };

      const layout = {
        title: "Monthly Sentiment & Tweet Volume",
        xaxis: { title: "Month" },
        yaxis: {
          title: "Average Sentiment",
          range: [-0.3, 0.3]
        },
        yaxis2: {
          title: "Tweet Volume",
          overlaying: 'y',
          side: 'right'
        },
        paper_bgcolor: "#fff",
        plot_bgcolor: "#fff"
      };

      Plotly.newPlot('textAnalysisChart', [sentimentTrace, volumeTrace], layout);
    }

    // Time Series Forecasting on Retail Sales
    if (document.getElementById('forecastChart')) {
      const months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun"];
      // Actual sales (units)
      const actualSales = [2000, 2200, 2500, 2300, 2800, 3100];
      // Predicted sales (units)
      const predSales = [1900, 2100, 2450, 2350, 2750, 3050];

      const actualTrace = {
        x: months,
        y: actualSales,
        name: "Actual Sales",
        type: "bar",
        marker: { color: "#007AFF" }
      };

      const predictedTrace = {
        x: months,
        y: predSales,
        name: "Predicted Sales",
        type: "bar",
        marker: { color: "#34C759" }
      };

      const layout = {
        title: "Actual vs. Predicted Retail Sales",
        barmode: "group",
        xaxis: { title: "Month" },
        yaxis: { title: "Sales (Units)" },
        paper_bgcolor: "#fff",
        plot_bgcolor: "#fff"
      };

      Plotly.newPlot('forecastChart', [actualTrace, predictedTrace], layout);
    }

    // Customer Segmentation from E-Commerce Logs
    if (document.getElementById('clusteringChart')) {
      // 4 clusters, each with purchase frequency (x) vs. avg spend (y)
      const clusterA = {
        x: [12, 14, 13, 11],
        y: [100, 120, 110, 105],
        mode: "markers",
        name: "Cluster A",
        marker: { color: "#FF2D55", size: 10 }
      };
      const clusterB = {
        x: [25, 28, 26, 27],
        y: [200, 210, 190, 220],
        mode: "markers",
        name: "Cluster B",
        marker: { color: "#FF9500", size: 10 }
      };
      const clusterC = {
        x: [40, 42, 45, 43],
        y: [450, 470, 440, 480],
        mode: "markers",
        name: "Cluster C",
        marker: { color: "#5856D6", size: 10 }
      };
      const clusterD = {
        x: [52, 55, 53, 56],
        y: [600, 620, 610, 630],
        mode: "markers",
        name: "Cluster D",
        marker: { color: "#34C759", size: 10 }
      };

      const layout = {
        title: "Customer Segments by Frequency & Avg Spend",
        xaxis: { title: "Purchases/Month" },
        yaxis: { title
